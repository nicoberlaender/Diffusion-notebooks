{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "HhiO558JxehQ"
      },
      "outputs": [],
      "source": [
        "%pip install torch\n",
        "%pip install diffusers\n",
        "%pip install transformers\n",
        "%pip install accelerate\n",
        "%pip install torchao"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "Wwpbr2_qxehS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import CogVideoXPipeline,AutoencoderKLCogVideoX, CogVideoXTransformer3DModel\n",
        "from diffusers.utils import export_to_video\n",
        "from transformers import T5EncoderModel\n",
        "from torchao.quantization import quantize_, int8_weight_only, int8_dynamic_activation_int8_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "9yS42HRnxehS"
      },
      "outputs": [],
      "source": [
        "quantization = int8_weight_only\n",
        "\n",
        "text_encoder = T5EncoderModel.from_pretrained(\"THUDM/CogVideoX-2b\", subfolder=\"text_encoder\", torch_dtype=torch.float16)\n",
        "quantize_(text_encoder, quantization())\n",
        "\n",
        "transformer = CogVideoXTransformer3DModel.from_pretrained(\"THUDM/CogVideoX-2b\", subfolder=\"transformer\", torch_dtype=torch.float16)\n",
        "quantize_(transformer, quantization())\n",
        "\n",
        "vae = AutoencoderKLCogVideoX.from_pretrained(\"THUDM/CogVideoX-2b\", subfolder=\"vae\", torch_dtype=torch.float16)\n",
        "quantize_(vae, quantization())\n",
        "\n",
        "# Create pipeline and run inference\n",
        "pipe = CogVideoXPipeline.from_pretrained(\n",
        "    \"THUDM/CogVideoX-2b\",\n",
        "    text_encoder=text_encoder,\n",
        "    transformer=transformer,\n",
        "    vae=vae,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "pipe.enable_model_cpu_offload()\n",
        "pipe.enable_sequential_cpu_offload()\n",
        "pipe.vae.enable_slicing()\n",
        "pipe.vae.enable_tiling()\n",
        "\n",
        "prompt = \"A panda, dressed in a small, red jacket and a tiny hat, sits on a wooden stool in a serene bamboo forest. The panda's fluffy paws strum a miniature acoustic guitar, producing soft, melodic tunes. Nearby, a few other pandas gather, watching curiously and some clapping in rhythm. Sunlight filters through the tall bamboo, casting a gentle glow on the scene. The panda's face is expressive, showing concentration and joy as it plays. The background includes a small, flowing stream and vibrant green foliage, enhancing the peaceful and magical atmosphere of this unique musical performance.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "I3X7oVszxehS"
      },
      "outputs": [],
      "source": [
        "video = pipe(\n",
        "    prompt=prompt,\n",
        "    num_videos_per_prompt=1,\n",
        "    num_inference_steps=50,\n",
        "    num_frames=49,\n",
        "    guidance_scale=6,\n",
        "    generator=torch.Generator(device=\"cuda\").manual_seed(42),\n",
        ").frames[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "kl3Xppa2xehS"
      },
      "outputs": [],
      "source": [
        "export_to_video(video, \"output.mp4\", fps=8)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}